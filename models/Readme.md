# ðŸ”¬ ConvNeXt: A Modern Convolutional Network

In our solution, we used **ConvNeXt**, a convolutional neural network re-designed with modern best practices and inspired by Vision Transformers (ViTs).  
ConvNeXt keeps the efficiency and inductive biases of CNNs, while borrowing design ideas from Transformers (patchify stem, large kernels, LayerNorm, inverted bottlenecks).


## ðŸ–¼ï¸ How ConvNeXt Works on an Input Image

Letâ€™s consider an **input image of size** `224 Ã— 224 Ã— 3` (RGB).  
ConvNeXt processes it in four main stages:

1. **Input**:  
   Shape = `(N, 3, 224, 224)`  

2. **Patchify Stem** (`Conv2d` with kernel=4, stride=4):  
   - Reduces spatial resolution: `224 â†’ 56`  
   - Expands channels: `3 â†’ 96`  (Number of output channels here are defined by the user: numberÂ ofÂ filtersÂ inÂ theÂ convolutionÂ layer)
   - Output shape = `(N, 96, 56, 56)`  

   *Formula*:  
  ```math
   H_{out} = \left\lfloor \frac{H - K}{S} \right\rfloor + 1
   ```
   For `H = 224, K = 4, S = 4` â†’ `56`.

3. **Stage Stack (Ã—4)**:  
   Each stage contains several **ConvNeXt blocks**.  
   Between stages, a downsampling layer halves resolution and doubles channels.

   - **Stage 1**: 3 blocks, channels = 96 â†’ `(N, 96, 56, 56)`  
   - **Stage 2**: Downsample + 3 blocks, channels = 192 â†’ `(N, 192, 28, 28)`  
   - **Stage 3**: Downsample + 9 blocks, channels = 384 â†’ `(N, 384, 14, 14)`  
   - **Stage 4**: Downsample + 3 blocks, channels = 768 â†’ `(N, 768, 7, 7)`  

4. **Head**:  
   - Global average pooling: `(N, 768, 7, 7) â†’ (N, 768)`  
   - Fully connected classifier: `(N, 768) â†’ (N, #classes)`  

---

## ðŸ§© Inside a ConvNeXt Block

A **ConvNeXt block** transforms `(N, C, H, W)` into `(N, C, H, W)` with residual learning:

```
Input (N, C, H, W)
â”œâ”€ Depthwise Conv (7Ã—7, groups=C) â†’ (N, C, H, W)
â”œâ”€ Permute â†’ (N, H, W, C)
â”œâ”€ LayerNorm
â”œâ”€ Linear (C â†’ 4C)
â”œâ”€ GELU
â”œâ”€ Linear (4C â†’ C)
â”œâ”€ LayerScale (Î³ âˆˆ â„^C, learnable, ~1e-6 init)
â”œâ”€ Permute back â†’ (N, C, H, W)
â”œâ”€ Residual connection (+ DropPath)
Output (N, C, H, W)
```


âœ… **Depthwise conv** = captures spatial context with large receptive field  
âœ… **MLP (1Ã—1 linear layers)** = mixes channel information efficiently  
âœ… **LayerNorm & GELU** = borrowed from Transformers for better training  
âœ… **Residual + DropPath** = stabilizes deep training  

---

### ðŸ”¢ Example with Numbers

Suppose input to **Stage 2 block** is `(N, 192, 28, 28)`.

1. **Depthwise Conv (7Ã—7)**  
   - Applies a spatial kernel per channel (no mixing across channels).  
   - Output shape = `(N, 192, 28, 28)`

   *Operation per channel \(c\)*:  
   ```math
   y_c(i,j) = \sum_{u=-3}^{3}\sum_{v=-3}^{3} W_c(u,v) \cdot x_c(i+u, j+v)
   ```

2. **LayerNorm (channel-wise)**  
   - Normalize each channel across `(H, W)` spatial positions.  
   - Keeps shape `(N, 28, 28, 192)` in NHWC format.

   *Formula*:  
   ```math
   \hat{x}_{c} = \frac{x_c - \mu_c}{\sqrt{\sigma_c^2 + \epsilon}}
   ```

3. **Linear Expansion (C â†’ 4C)**  
   - `(192 â†’ 768)`  
   - Shape = `(N, 28, 28, 768)`

4. **GELU Activation**  
   ```math
   \text{GELU}(x) = 0.5x \left( 1 + \tanh\!\left(\sqrt{\tfrac{2}{\pi}} (x + 0.044715x^3)\right)\right)
   ```

5. **Linear Projection (4C â†’ C)**  
   - `(768 â†’ 192)`  
   - Shape = `(N, 28, 28, 192)`

6. **LayerScale**  
   - Multiply output by learnable Î³ âˆˆ â„^192 (initial ~1e-6).  
   - Helps stabilize deep training.

7. **Residual + DropPath**  
   - Add input `(N, 28, 28, 192)` to output.  
   - DropPath stochastically drops residual branches during training.

âœ… Final output shape remains `(N, 192, 28, 28)`.

---

## ðŸ“Š Size Summary (ConvNeXt-Tiny Example)

| Stage   | Resolution | Channels | Blocks |
|---------|------------|----------|--------|
| Input   | 224Ã—224    | 3        | â€“      |
| Stem    | 56Ã—56      | 96       | â€“      |
| Stage 1 | 56Ã—56      | 96       | 3      |
| Stage 2 | 28Ã—28      | 192      | 3      |
| Stage 3 | 14Ã—14      | 384      | 9      |
| Stage 4 | 7Ã—7        | 768      | 3      |
| Head    | 1Ã—1        | 768      | â€“      |

---

## ðŸ“Š Why ConvNeXt?

- Matches or outperforms Vision Transformers while staying purely convolutional  
- Scales efficiently across different model sizes (Tiny â†’ Large)  
- Simple, unified design that combines the best of CNNs and Transformers  

---

## ðŸ“– Reference

ConvNeXt was introduced in:  
> **A ConvNet for the 2020s**  
> Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie  
> CVPR 2022 â€” [Paper](https://arxiv.org/abs/2201.03545)

