# ðŸ”¬ ConvNeXt: A Modern Convolutional Network

In our solution, we used **ConvNeXt**, a convolutional neural network re-designed with modern best practices and inspired by Vision Transformers (ViTs).  
ConvNeXt keeps the efficiency and inductive biases of CNNs, while borrowing design ideas from Transformers (patchify stem, large kernels, LayerNorm, inverted bottlenecks).


## ðŸ–¼ï¸ How ConvNeXt Works on an Input Image

1. **Input**: RGB image (e.g. `224Ã—224Ã—3`)  
2. **Patchify Stem**: A `4Ã—4` convolution with stride 4 splits the image into non-overlapping patches (like ViTs), reducing spatial size (224 â†’ 56).  
3. **Stages (Ã—4)**:  
   - Each stage contains multiple **ConvNeXt blocks**  
   - Between stages, downsampling halves resolution and doubles channels  
   - Typical channel sizes: `[96, 192, 384, 768]`  
4. **Head**: Global average pooling â†’ Linear classifier â†’ Predictions  


## ðŸ§© Inside a ConvNeXt Block

Each block follows a simple but powerful design:
```
Input (N, C, H, W)
â”œâ”€ Depthwise Conv (7Ã—7) â†’ spatial mixing
â”œâ”€ LayerNorm
â”œâ”€ Linear (C â†’ 4C) + GELU + Linear (4C â†’ C) â†’ channel mixing
â”œâ”€ LayerScale (tiny learnable Î³)
â”œâ”€ Residual connection (+ DropPath)
Output (N, C, H, W)
```


âœ… **Depthwise conv** = captures spatial context with large receptive field  
âœ… **MLP (1Ã—1 linear layers)** = mixes channel information efficiently  
âœ… **LayerNorm & GELU** = borrowed from Transformers for better training  
âœ… **Residual + DropPath** = stabilizes deep training  

---

## ðŸ“Š Why ConvNeXt?

- Matches or outperforms Vision Transformers while staying purely convolutional  
- Scales efficiently across different model sizes (Tiny â†’ Large)  
- Simple, unified design that combines the best of CNNs and Transformers  

---

## ðŸ“– Reference

ConvNeXt was introduced in:  
> **A ConvNet for the 2020s**  
> Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie  
> CVPR 2022 â€” [Paper](https://arxiv.org/abs/2201.03545)

